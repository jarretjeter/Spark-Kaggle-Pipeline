{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 14:21:15 WARN Utils: Your hostname, DESKTOP-EJLBN3A resolves to a loopback address: 127.0.1.1; using 172.23.236.23 instead (on interface eth0)\n",
      "22/07/01 14:21:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jarret/data-engineering-bootcamp/workspace/spark-kaggle-pipeline/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/01 14:21:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Open', 'float'),\n",
       " ('High', 'float'),\n",
       " ('Low', 'float'),\n",
       " ('Close', 'float'),\n",
       " ('Volume', 'float'),\n",
       " ('Currency', 'string')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Up the Data\n",
    "\n",
    "from pyspark.sql.types import DateType, FloatType\n",
    "\n",
    "coffee_df = spark.read.csv(\"./coffee.csv\", header=True)\n",
    "\n",
    "# change the column data types\n",
    "coffee_df = coffee_df.withColumn(\"Date\", coffee_df.Date.cast(DateType()))\n",
    "coffee_df = coffee_df.withColumn(\"Open\", coffee_df.Open.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"High\", coffee_df.High.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Low\", coffee_df.Low.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Close\", coffee_df.Close.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Volume\", coffee_df.Volume.cast(FloatType()))\n",
    "coffee_df.show(10)\n",
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+----------+---------+-----------+------------------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|Open_Close| High_Low|Vol_100plus|   Open_Close(abs)|\n",
      "+----------+------+------+------+------+-------+--------+----------+---------+-----------+------------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|      5.75|7.9000015|       true|              5.75|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|       0.0|     4.75|       true|               0.0|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|-3.5999985|      6.0|       true|3.5999984741210938|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD| 2.1500015|4.9000015|       true|2.1500015258789062|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD| 3.0999985| 3.949997|       true|3.0999984741210938|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|  5.949997| 9.300003|       true|5.9499969482421875|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD| -2.300003|     2.75|       true|2.3000030517578125|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|-1.1499939|3.5999985|       true| 1.149993896484375|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|0.69999695|      2.5|       true|0.6999969482421875|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|  5.199997|      8.0|       true|5.1999969482421875|\n",
      "+----------+------+------+------+------+-------+--------+----------+---------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+---------+--------+----------+\n",
      "|Open(avg)|High(avg)|Low(avg)|Close(avg)|\n",
      "+---------+---------+--------+----------+\n",
      "|   126.05|   127.61|  124.59|    125.99|\n",
      "+---------+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns from Aggregate Functions\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "def abs_col_values(num: int or float) -> int or float:\n",
    "    \"returns the absolute values of a column\"\n",
    "    return abs(num)\n",
    "\n",
    "udf_abs_col_values = sf.udf(lambda x: abs_col_values(x))\n",
    "\n",
    "coffee_df = coffee_df.withColumn(\"Open_Close\", coffee_df.Open - coffee_df.Close)\n",
    "coffee_df = coffee_df.withColumn(\"High_Low\", coffee_df.High - coffee_df.Low)\n",
    "coffee_df = coffee_df.withColumn(\"Vol_100plus\", coffee_df.Volume >= 100)\n",
    "coffee_df = coffee_df.withColumn(\"Open_Close(abs)\", udf_abs_col_values(coffee_df.Open_Close))\n",
    "\n",
    "coffee_df_avgs = coffee_df.agg(\n",
    "    sf.round(sf.avg(\"Open\"), 2).alias(\"Open(avg)\"), \n",
    "    sf.round(sf.avg(\"High\"), 2).alias(\"High(avg)\"), \n",
    "    sf.round(sf.avg(\"Low\"), 2).alias(\"Low(avg)\"), \n",
    "    sf.round(sf.avg(\"Close\"), 2).alias(\"Close(avg)\")\n",
    "    )\n",
    "coffee_df.show(10)\n",
    "coffee_df_avgs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+----------+---------+-----------+------------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close| High_Low|Vol_100plus|   Open_Close(abs)|\n",
      "+----------+------+------+------+------+------+--------+----------+---------+-----------+------------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|6640.0|     USD|      5.75|7.9000015|       true|              5.75|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|5492.0|     USD|       0.0|     4.75|       true|               0.0|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|6165.0|     USD|-3.5999985|      6.0|       true|3.5999984741210938|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|5094.0|     USD| 2.1500015|4.9000015|       true|2.1500015258789062|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|6855.0|     USD| 3.0999985| 3.949997|       true|3.0999984741210938|\n",
      "+----------+------+------+------+------+------+--------+----------+---------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|avg(Open_Close(abs))|\n",
      "+--------------------+\n",
      "|  1.7606027822995378|\n",
      "+--------------------+\n",
      "\n",
      "1638\n",
      "+-----------------+\n",
      "|        avg(Open)|\n",
      "+-----------------+\n",
      "|126.0496775257701|\n",
      "+-----------------+\n",
      "\n",
      "+---------+\n",
      "|max(High)|\n",
      "+---------+\n",
      "|   306.25|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stats\n",
    "\n",
    "coffee_df.show(5)\n",
    "\n",
    "# average of the value difference between the Open and Close columns\n",
    "open_close_avg = coffee_df.agg(sf.avg(\"Open_Close(abs)\"))\n",
    "open_close_avg.show()\n",
    "\n",
    "# the number of times that Volume's values were less than 100\n",
    "volume_under_100 = coffee_df.filter(\"Volume < 100\").count()\n",
    "print(volume_under_100)\n",
    "\n",
    "# the average opening price\n",
    "open_avg = coffee_df.agg(sf.avg(\"Open\"))\n",
    "open_avg.show()\n",
    "\n",
    "# the max high value\n",
    "max_high = coffee_df.agg(sf.max(\"High\"))\n",
    "max_high.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d47fd02957810a16bbf43d777a275107958cdb917160b0d1be38a7f1cd13073a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
