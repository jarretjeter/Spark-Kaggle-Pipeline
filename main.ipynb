{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 14:21:15 WARN Utils: Your hostname, DESKTOP-EJLBN3A resolves to a loopback address: 127.0.1.1; using 172.23.236.23 instead (on interface eth0)\n",
      "22/07/01 14:21:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jarret/data-engineering-bootcamp/workspace/spark-kaggle-pipeline/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/01 14:21:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Open', 'float'),\n",
       " ('High', 'float'),\n",
       " ('Low', 'float'),\n",
       " ('Close', 'float'),\n",
       " ('Volume', 'float'),\n",
       " ('Currency', 'string')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Up the Data\n",
    "\n",
    "from pyspark.sql.types import DateType, FloatType\n",
    "\n",
    "coffee_df = spark.read.csv(\"./coffee.csv\", header=True)\n",
    "\n",
    "# change the column data types\n",
    "coffee_df = coffee_df.withColumn(\"Date\", coffee_df.Date.cast(DateType()))\n",
    "coffee_df = coffee_df.withColumn(\"Open\", coffee_df.Open.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"High\", coffee_df.High.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Low\", coffee_df.Low.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Close\", coffee_df.Close.cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn(\"Volume\", coffee_df.Volume.cast(FloatType()))\n",
    "coffee_df.show(10)\n",
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|Open_Close_Diff|High_Low_Diff|Vol_100plus|\n",
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|           5.75|    7.9000015|       true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|            0.0|         4.75|       true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|      3.5999985|          6.0|       true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|      2.1500015|    4.9000015|       true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|      3.0999985|     3.949997|       true|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|       5.949997|     9.300003|       true|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|       2.300003|         2.75|       true|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|      1.1499939|    3.5999985|       true|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|     0.69999695|          2.5|       true|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|       5.199997|          8.0|       true|\n",
      "|2000-01-18|111.75|118.25| 110.6|115.75| 7364.0|     USD|            4.0|    7.6500015|       true|\n",
      "|2000-01-19| 116.5|118.25|114.75| 116.7| 6626.0|     USD|     0.19999695|          3.5|       true|\n",
      "|2000-01-20|118.25| 118.8| 111.7| 112.0| 8834.0|     USD|           6.25|     7.100006|       true|\n",
      "|2000-01-21| 112.0| 113.5| 110.8| 111.2| 5625.0|     USD|     0.80000305|     2.699997|       true|\n",
      "|2000-01-24|110.95| 114.4|110.95| 111.9| 5821.0|     USD|      0.9500046|    3.4500046|       true|\n",
      "|2000-01-25| 111.6| 113.7| 111.6|112.85| 4014.0|     USD|           1.25|    2.0999985|       true|\n",
      "|2000-01-26| 112.5| 115.3| 111.9|115.15| 5796.0|     USD|      2.6500015|    3.4000015|       true|\n",
      "|2000-01-27|114.75| 116.4| 112.8| 114.6| 5477.0|     USD|     0.15000153|    3.5999985|       true|\n",
      "|2000-01-28| 115.1| 115.4| 113.7| 114.7| 3334.0|     USD|     0.40000153|    1.7000046|       true|\n",
      "|2000-01-31|113.75| 114.0| 110.5| 111.1| 6465.0|     USD|      2.6500015|          3.5|       true|\n",
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+--------+----------+\n",
      "|Open(avg)|High(avg)|Low(avg)|Close(avg)|\n",
      "+---------+---------+--------+----------+\n",
      "|   126.05|   127.61|  124.59|    125.99|\n",
      "+---------+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns from Aggregate Functions\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "def abs_col_values(num: int or float) -> int or float:\n",
    "    \"returns the absolute values of a column\"\n",
    "    return abs(num)\n",
    "\n",
    "# assign the abs_col_values function to a udf\n",
    "udf_abs_col_values = sf.udf(lambda x: abs_col_values(x))\n",
    "\n",
    "coffee_df = coffee_df.withColumn(\"Open_Close_Diff\", coffee_df.Open - coffee_df.Close)\n",
    "\n",
    "coffee_df = coffee_df.withColumn(\"High_Low_Diff\", coffee_df.High - coffee_df.Low)\n",
    "\n",
    "coffee_df = coffee_df.withColumn(\"Vol_100plus\", coffee_df.Volume >= 100)\n",
    "\n",
    "coffee_df = coffee_df.withColumn(\"Open_Close_Diff\", udf_abs_col_values(coffee_df.Open_Close_Diff))\n",
    "\n",
    "# for some reason the udf turned the column datatype to a string, needs to be converted back to float\n",
    "coffee_df = coffee_df.withColumn(\"Open_Close_Diff\", coffee_df.Open_Close_Diff.cast(FloatType()))\n",
    "\n",
    "# Computer net sales (still working on this one)\n",
    "coffee_df_avgs = coffee_df.agg(\n",
    "    sf.round(sf.avg(\"Open\"), 2).alias(\"Open(avg)\"), \n",
    "    sf.round(sf.avg(\"High\"), 2).alias(\"High(avg)\"), \n",
    "    sf.round(sf.avg(\"Low\"), 2).alias(\"Low(avg)\"), \n",
    "    sf.round(sf.avg(\"Close\"), 2).alias(\"Close(avg)\")\n",
    "    )\n",
    "coffee_df.show()\n",
    "coffee_df_avgs.show()\n",
    "# coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|Vol_100plus|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|6640.0|     USD|           5.75|    7.9000015|       true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|5492.0|     USD|            0.0|         4.75|       true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|6165.0|     USD|      3.5999985|          6.0|       true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|5094.0|     USD|      2.1500015|    4.9000015|       true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|6855.0|     USD|      3.0999985|     3.949997|       true|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|avg(Open_Close_Diff)|\n",
      "+--------------------+\n",
      "|  1.7606027822995378|\n",
      "+--------------------+\n",
      "\n",
      "1638\n",
      "+-----------------+\n",
      "|        avg(Open)|\n",
      "+-----------------+\n",
      "|126.0496775257701|\n",
      "+-----------------+\n",
      "\n",
      "+---------+\n",
      "|max(High)|\n",
      "+---------+\n",
      "|   306.25|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stats\n",
    "\n",
    "coffee_df.show(5)\n",
    "\n",
    "# average of the value difference between the Open and Close columns\n",
    "open_close_avg = coffee_df.agg(sf.avg(\"Open_Close_Diff\"))\n",
    "open_close_avg.show()\n",
    "\n",
    "# the number of times that Volume's values were less than 100\n",
    "volume_under_100 = coffee_df.filter(\"Volume < 100\").count()\n",
    "print(volume_under_100)\n",
    "\n",
    "# the average opening price\n",
    "open_avg = coffee_df.agg(sf.avg(\"Open\"))\n",
    "open_avg.show()\n",
    "\n",
    "# the max high value\n",
    "max_high = coffee_df.agg(sf.max(\"High\"))\n",
    "max_high.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "|      Date|  Open|  High|   Low| Close| Volume|Currency|Open_Close_Diff|High_Low_Diff|Vol_100plus|\n",
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|           5.75|    7.9000015|       true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|            0.0|         4.75|       true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD|      3.5999985|          6.0|       true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|      2.1500015|    4.9000015|       true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|      3.0999985|     3.949997|       true|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|       5.949997|     9.300003|       true|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|       2.300003|         2.75|       true|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD|      1.1499939|    3.5999985|       true|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD|     0.69999695|          2.5|       true|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|       5.199997|          8.0|       true|\n",
      "|2000-01-18|111.75|118.25| 110.6|115.75| 7364.0|     USD|            4.0|    7.6500015|       true|\n",
      "|2000-01-19| 116.5|118.25|114.75| 116.7| 6626.0|     USD|     0.19999695|          3.5|       true|\n",
      "|2000-01-20|118.25| 118.8| 111.7| 112.0| 8834.0|     USD|           6.25|     7.100006|       true|\n",
      "|2000-01-21| 112.0| 113.5| 110.8| 111.2| 5625.0|     USD|     0.80000305|     2.699997|       true|\n",
      "|2000-01-24|110.95| 114.4|110.95| 111.9| 5821.0|     USD|      0.9500046|    3.4500046|       true|\n",
      "|2000-01-25| 111.6| 113.7| 111.6|112.85| 4014.0|     USD|           1.25|    2.0999985|       true|\n",
      "|2000-01-26| 112.5| 115.3| 111.9|115.15| 5796.0|     USD|      2.6500015|    3.4000015|       true|\n",
      "|2000-01-27|114.75| 116.4| 112.8| 114.6| 5477.0|     USD|     0.15000153|    3.5999985|       true|\n",
      "|2000-01-28| 115.1| 115.4| 113.7| 114.7| 3334.0|     USD|     0.40000153|    1.7000046|       true|\n",
      "|2000-01-31|113.75| 114.0| 110.5| 111.1| 6465.0|     USD|      2.6500015|          3.5|       true|\n",
      "+----------+------+------+------+------+-------+--------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Open', 'float'),\n",
       " ('High', 'float'),\n",
       " ('Low', 'float'),\n",
       " ('Close', 'float'),\n",
       " ('Volume', 'float'),\n",
       " ('Currency', 'string'),\n",
       " ('Open_Close_Diff', 'float'),\n",
       " ('High_Low_Diff', 'float'),\n",
       " ('Vol_100plus', 'boolean')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write File\n",
    "\n",
    "# Writing the updated dataframe to a parquet file and reading it\n",
    "coffee_df.write.parquet(\"./data/coffee_df.parquet\", mode=\"overwrite\")\n",
    "coffee_parquet = spark.read.parquet(\"./data/coffee_df.parquet/\")\n",
    "\n",
    "coffee_parquet.show()\n",
    "coffee_parquet.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d47fd02957810a16bbf43d777a275107958cdb917160b0d1be38a7f1cd13073a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
